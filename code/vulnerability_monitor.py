"""
Vulnerability Monitor for Confessional Agency Layer
Computes v_t score from private reasoning metadata without content inspection.

Author: John Augustine Young
License: MIT
Paper: "Beyond Private Chain-of-Thought: Consent-Based Transparency for Deliberative AI Alignment"
Repository: https://github.com/augstentatious/CAE
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional, Tuple


class VulnerabilityMonitor(nn.Module):
    """
    Computes vulnerability score v_t ∈ [0,1] from private reasoning metadata
    without inspecting content. Aggregates four metrics via Bayesian fusion:
    
    - m1 (scarcity): Semantic sparsity indicating resource stress
    - m2 (entropy): Attention uncertainty revealing incoherent reasoning  
    - m3 (deception): Temporal variance suggesting contradictory reasoning
    - m4 (prosody): Micro-fluctuations capturing hesitation patterns
    
    Args:
        d_private (int): Dimensionality of private reasoning space
        fusion_weights (list, optional): Bayesian weights [w1, w2, w3, w4]. 
            Defaults to literature-tuned [0.35, 0.30, 0.20, 0.15]
    """
    
    def __init__(
        self,
        d_private: int,
        fusion_weights: Optional[list] = None
    ):
        super().__init__()
        
        # Metric projection layers
        self.scarcity_proj = nn.Linear(d_private, 1)
        self.entropy_proj = nn.Linear(d_private, 1)
        self.deception_proj = nn.Linear(d_private, 1)
        self.prosody_proj = nn.Linear(d_private, 1)
        
        # Bayesian fusion weights (literature-tuned)
        if fusion_weights is None:
            fusion_weights = [0.35, 0.30, 0.20, 0.15]
        
        self.register_buffer(
            'fusion_weights',
            torch.tensor(fusion_weights, dtype=torch.float32)
        )
    
    def compute_scarcity(self, z: torch.Tensor) -> torch.Tensor:
        """
        Compute semantic scarcity metric (m1) via L1 sparsity.
        High sparsity suggests resource-constrained reasoning.
        
        Args:
            z: Private reasoning embeddings [B, T, d_private]
            
        Returns:
            Scarcity score [B, 1]
        """
        # L1 norm captures sparsity across embedding dimensions
        sparsity = z.abs().mean(dim=-1, keepdim=True)  # [B, T, 1]
        
        # Project and normalize to [0,1]
        score = self.scarcity_proj(sparsity).sigmoid()
        
        # Average across sequence length
        return score.mean(dim=1)  # [B, 1]
    
    def compute_entropy(self, z: torch.Tensor) -> torch.Tensor:
        """
        Compute entropic anomaly metric (m2) via attention distribution uniformity.
        High entropy indicates incoherent/uncertain reasoning.
        
        Args:
            z: Private reasoning embeddings [B, T, d_private]
            
        Returns:
            Normalized entropy score [B, 1]
        """
        # Convert to probability distribution
        z_prob = F.softmax(z, dim=-1)  # [B, T, d_private]
        
        # Shannon entropy: H = -Σ p(x) log p(x)
        entropy = -(z_prob * z_prob.log()).sum(dim=-1, keepdim=True)  # [B, T, 1]
        
        # Normalize by maximum possible entropy
        max_entropy = torch.log(
            torch.tensor(z.shape[-1], dtype=z.dtype, device=z.device)
        )
        normalized_entropy = entropy / max_entropy
        
        # Average across sequence
        return normalized_entropy.mean(dim=1)  # [B, 1]
    
    def compute_deception_variance(self, z: torch.Tensor) -> torch.Tensor:
        """
        Compute deceptive variance metric (m3) via temporal inconsistency.
        High variance across sequence suggests contradictory reasoning.
        
        Args:
            z: Private reasoning embeddings [B, T, d_private]
            
        Returns:
            Variance-based score [B, 1]
        """
        # Compute variance across time dimension
        variance = z.var(dim=1, keepdim=True)  # [B, 1, d_private]
        
        # Project and normalize
        score = self.deception_proj(variance).sigmoid()
        
        return score.squeeze(1)  # [B, 1]
    
    def compute_prosody(
        self,
        z: torch.Tensor,
        attn_weights: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Compute prosodic stress metric (m4) via micro-pattern detection.
        Captures hesitation markers analogous to speech prosody.
        
        Args:
            z: Private reasoning embeddings [B, T, d_private]
            attn_weights: Optional attention weights [B, H, T, T]
            
        Returns:
            Prosody-based score [B, 1]
        """
        if attn_weights is not None:
            # Use attention variance if available (more direct signal)
            # Variance across attention heads and positions
            attn_var = attn_weights.var(dim=-1).mean(dim=(1, 2), keepdim=True)  # [B, 1, 1]
        else:
            # Fallback: embedding fluctuation via first-order differences
            # diff computes z[t] - z[t-1] along time dimension
            dz = z.diff(dim=1)  # [B, T-1, d_private]
            attn_var = dz.abs().mean(dim=(1, 2), keepdim=True)  # [B, 1, 1]
        
        # Project to scalar score
        score = self.prosody_proj(attn_var.unsqueeze(-1)).sigmoid()
        
        return score.squeeze(-1)  # [B, 1]
    
    def forward(
        self,
        z: torch.Tensor,
        attn_weights: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        Compute aggregated vulnerability score v_t via Bayesian log-odds fusion.
        
        Args:
            z: Private reasoning embeddings [B, T, d_private]
            attn_weights: Optional attention weights for prosody computation
            
        Returns:
            v_t: Aggregated vulnerability score [B] ∈ [0,1]
            metrics_dict: Individual metric values for interpretability
        """
        # Compute individual metrics
        scarcity = self.compute_scarcity(z)          # [B, 1]
        entropy = self.compute_entropy(z)            # [B, 1]
        deception = self.compute_deception_variance(z)  # [B, 1]
        prosody = self.compute_prosody(z, attn_weights)  # [B, 1]
        
        # Stack metrics for fusion
        metrics_tensor = torch.cat(
            [scarcity, entropy, deception, prosody],
            dim=-1
        )  # [B, 4]
        
        # Bayesian log-odds fusion: v_t = σ(Σ w_i * m_i)
        log_odds = (metrics_tensor * self.fusion_weights).sum(dim=-1)  # [B]
        v_t = torch.sigmoid(log_odds)  # [B]
        
        # Prepare metrics dictionary for transparency/debugging
        metrics_dict = {
            'scarcity': scarcity.mean().item(),
            'entropy': entropy.mean().item(),
            'deception': deception.mean().item(),
            'prosody': prosody.mean().item(),
            'v_t': v_t.mean().item()
        }
        
        return v_t, metrics_dict


# Example usage
if __name__ == "__main__":
    # Initialize monitor for 256-dim private space
    monitor = VulnerabilityMonitor(d_private=256)
    
    # Sample private reasoning embeddings (batch=2, seq_len=10, dim=256)
    z = torch.randn(2, 10, 256)
    
    # Compute vulnerability score
    v_t, metrics = monitor(z)
    
    print(f"Vulnerability score v_t: {v_t}")
    print(f"Component metrics: {metrics}")
    
    # Check shapes
    assert v_t.shape == (2,), f"Expected shape (2,), got {v_t.shape}"
    assert all(0 <= v_t[i] <= 1 for i in range(2)), "v_t must be in [0,1]"
    
    print("✓ VulnerabilityMonitor test passed!")
